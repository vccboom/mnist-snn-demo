{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image classification through Spiking Neural Networks\n",
    "This notebook presents the step-by-step development of an impulse neural network.\n",
    "It is designed as part of the article: \"From the Brain to the Machine: Digital Synapses with Spiking Neural Networks\", for the scientific dissemination magazine on artificial intelligence, Komputer Sapiens. However, it is not necessary to have read this article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries\n",
    "- import torch: Imports PyTorch, a deep learning library used to create and train neural networks.\n",
    "\n",
    "- import torch.nn as nn: Imports the nn module from PyTorch, which contains tools for defining and working with neural network layers and functions.\n",
    "\n",
    "- import snntorch as snn: provides tools for working with Spiking Neural Networks in PyTorch.\n",
    "\n",
    "- import matplotlib.pyplot as plt: A library for creating visualizations such as plots and charts.\n",
    "\n",
    "- from torchvision import datasets, transforms: Imports modules from torchvision for loading and transforming image datasets commonly used in computer vision tasks.\n",
    "\n",
    "- from torch.utils.data import DataLoader: Imports DataLoader, which facilitates loading and managing data in mini-batches during training.\n",
    "\n",
    "- from snntorch import spikegen: Used for generating spike-based input data for spiking neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import snntorch as snn\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms #\n",
    "#from snntorch import utils\n",
    "from torch.utils.data import DataLoader\n",
    "from snntorch import spikegen\n",
    "#import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section of the code sets up the environment and prepares the dataset for processing. The `data_path` variable specifies the location where the MNIST dataset will be stored. It defines the data type for tensors as `torch.float` to optimize memory usage and compatibility with certain libraries. The `device` variable is configured to use a GPU if available (CUDA for NVIDIA GPUs or MPS for Apple Silicon), or defaults to the CPU otherwise. Finally, the `transform` variable creates a series of transformations applied to the dataset, including converting images to PyTorch tensors and normalizing them to have a mean of 0 and a standard deviation of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path='/tmp/data/mnist'\n",
    "\n",
    "# Torch Variables\n",
    "dtype = torch.float       \n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0,), (1,))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet handles the loading and batching of the MNIST dataset. It sets the `batch_size` to 160 and loads the MNIST training and test datasets using the `datasets.MNIST` function, applying the previously defined transformations. Next, `DataLoader` objects are created for both the training and test datasets with the specified batch size and shuffling enabled to ensure random sampling of data. The sizes of these `DataLoader` objects indicate the number of batches in each loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of mnist_train is 60000\n",
      "The size of mnist_test is 10000\n",
      "The size of train_loader is 375\n",
      "The size of test_loader is 63\n"
     ]
    }
   ],
   "source": [
    "batch_size=160\n",
    "mnist_train = datasets.MNIST(data_path, train=True, download=True, transform=transform)\n",
    "mnist_test = datasets.MNIST(data_path, train=False, download=True, transform=transform)\n",
    "print(f\"The size of mnist_train is {len(mnist_train)}\")\n",
    "print(f\"The size of mnist_test is {len(mnist_test)}\")\n",
    "train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=True)\n",
    "print(f\"The size of train_loader is {len(train_loader)}\")\n",
    "print(f\"The size of test_loader is {len(test_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Architecture\n",
    "num_inputs = 28*28    \n",
    "num_hidden = 64                 # You can increase the number of neurons\n",
    "num_outputs = 10\n",
    "\n",
    "# Temporal Dynamics\n",
    "num_steps = 30                  #You can also change the number of time steps, 30, 50,\n",
    "beta = 0.95                     #60, etc... but consider that you add more computational cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section of the code defines a spiking neural network (SNN) architecture using PyTorch. The Net class inherits from `nn.Module` and initializes three layers: `fc1`, `fc2`, and `fc3`, which are fully connected layers for the input, hidden, and output layers, respectively. Each fully connected layer is followed by a snn.Leaky layer, which creates leaky integrate-and-fire (LIF) neurons, simulating spiking neural behavior.\n",
    "\n",
    "At each time step:\n",
    "\n",
    "- `cur1`, `cur2`, and `cur3` represent the current inputs to the layers (fc1, fc2, and fc3) respectively, with dimensions `[batch_size, num_hidden]`, `[batch_size, 64]`, and `[batch_size, num_outputs]`.\n",
    "- `spk1`, `spk2`, and `spk3` represent the spike outputs from the LIF neurons. These are binary arrays (0 or 1) corresponding to the neurons firing in each layer, sharing the same dimensions as cur1, cur2, and cur3.\n",
    "- `mem1`, `mem2`, and `mem3` are the membrane potentials of the LIF neurons, representing the internal state of the neuron. These tensors have the same dimensions as the spike outputs, capturing the voltage over time.\n",
    "The network's output is a sequence of tensors (`spk3_rec` and `mem3_rec`) stacked along the time dimension, resulting in dimensions `[num_steps, batch_size, num_outputs]`. These outputs capture the temporal evolution of spikes and membrane potentials across all layers of the network.\n",
    "\n",
    "Finally, the network is loaded onto the available computational device, such as CUDA (GPU) if available, for accelerated processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize layers\n",
    "        self.fc1 = nn.Linear(num_inputs, num_hidden)\n",
    "        self.lif1 = snn.Leaky(beta=beta)     \n",
    "        self.fc2= nn.Linear(num_hidden, 64)  # Capa adicional\n",
    "        self.lif2 = snn.Leaky(beta=beta)               \n",
    "        self.fc3 = nn.Linear(64, num_outputs)\n",
    "        self.lif3 = snn.Leaky(beta=beta)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden states at t=0\n",
    "        mem1 = self.lif1.init_leaky()                                \n",
    "        mem2 = self.lif2.init_leaky()    \n",
    "        mem3 = self.lif3.init_leaky()                           \n",
    "\n",
    "        # Record the final layer\n",
    "        spk3_rec = []\n",
    "        mem3_rec = []\n",
    "        for step in range(num_steps):\n",
    "            cur1 = self.fc1(x[step])\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)     #Aquí spk1 es el output donde en este caso es de tamaño batch, input_size. Es un array de 0s y 1s y para está dentro de un for de 25=num_steps de tiempo y mem es el potencial de membrana siguiente para cada elemento en el batch\n",
    "            cur2 = self.fc2(spk1)\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "            cur3 = self.fc3(spk2)\n",
    "            spk3, mem3 = self.lif3(cur3, mem3)\n",
    "            spk3_rec.append(spk3)\n",
    "            mem3_rec.append(mem3)\n",
    "        return torch.stack(spk3_rec, dim=0), torch.stack(mem3_rec, dim=0)      # Son una secuencia de tensores y los apila en la dimensión 0 (tiempo). Dimensiones: mem_rec = [num_steps, batch_size, num_outputs]\n",
    "\n",
    "# Load the network onto CUDA if available\n",
    "net = Net().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the code, the loss function and optimizer are set up for training the spiking neural network. \n",
    "\n",
    "- `CE_loss = nn.CrossEntropyLoss()` initializes the cross-entropy loss function. It compares the network's output predictions with the true labels and calculates a scalar value representing how far the predictions are from the actual classes.\n",
    "\n",
    "- `optimizer = torch.optim.Adam(net.parameters(), lr=5e-4, betas=(0.9, 0.999))` sets up the Adam optimizer, a popular choice for training neural networks due to its ability to adapt the learning rate for each parameter. The learning rate (`lr`) is set to `5e-4`, and `betas` control the exponential decay rates for the moment estimates. This optimizer will adjust the network's parameters during backpropagation to minimize the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "CE_loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=5e-4, betas=(0.9, 0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Parameters and Model Training Loop\n",
    "\n",
    "This code block defines the training parameters and implements the training loop for the spiking neural network.\n",
    "\n",
    "- **Training Parameters**: \n",
    "  - `num_epochs = 1`: The network will train for 1 epoch.\n",
    "  - `num_classes = 10`: Represents the number of output classes.\n",
    "  - `loss_hist`: A list initialized to store the history of training loss values across epochs.\n",
    "\n",
    "- **Model Training Loop**: \n",
    "  - The loop iterates over each epoch. For each batch of data in the `train_loader`, the following steps are executed:\n",
    "    1. **Latency Coding**: `spike_data = spikegen.latency(data, num_steps=num_steps)` converts the input data into spike trains using latency coding, where the timing of spikes is inversely proportional to the input intensity.\n",
    "    2. **Data Preparation**: The spike data and targets are moved to the device (CPU, CUDA, or MPS) and reshaped to ensure correct dimensionality for the network.\n",
    "    3. **Forward Pass**: The network's forward method processes the spike data, returning outputs and membrane potential recordings (`mem_rec`). The network is optimized based on the error between the predicted and actual labels.\n",
    "    4. **Loss Calculation**: The loss value (`loss_val`) is accumulated over the `num_steps` time steps by applying the cross-entropy loss (`CE_loss`) to the membrane potential outputs at each time step.\n",
    "    5. **Backpropagation and Optimization**: The accumulated loss is backpropagated, and the optimizer updates the network parameters to minimize the loss.\n",
    "    6. **Logging**: The loss for the current epoch is stored and printed.\n",
    "\n",
    "This loop trains the spiking neural network by iterating over the entire training dataset and updating the network's weights based on the loss calculated from the membrane potentials at each time step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Loss: 26.5226\n"
     ]
    }
   ],
   "source": [
    "# Training Parameters\n",
    "num_epochs = 1\n",
    "num_classes = 10  \n",
    "loss_hist = []\n",
    "test_loss_hist = []\n",
    "counter = 0\n",
    "# Entrenamiento del modelo\n",
    "for epoch in range(num_epochs):\n",
    "    net.train()\n",
    "    for data, targets in train_loader:\n",
    "        spike_data = spikegen.latency(data, num_steps=num_steps)\n",
    "        spike_data = spike_data.to(device)                            \n",
    "        targets = targets.to(device) \n",
    "        spike_data = spike_data.view(num_steps, data.size(0), -1)\n",
    "        optimizer.zero_grad()\n",
    "        outputs, mem_rec = net(spike_data)\n",
    "\n",
    "        loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
    "        for step in range(num_steps):\n",
    "            loss_val += CE_loss(mem_rec[step], targets)           \n",
    "        loss_hist.append(loss_val.item())\n",
    "        loss_val.backward()\n",
    "        optimizer.step()  \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss_val.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Spiking Neural Network Model\n",
    "\n",
    "This code snippet is responsible for evaluating the trained Spiking Neural Network (SNN) on the test dataset to determine its accuracy in classifying the MNIST digits.\n",
    "\n",
    "1. **Set the Network to Evaluation Mode**: \n",
    "   - The `net.eval()` function is called to switch the network to evaluation mode. It ensures that layers like dropout, which behave differently during training, are turned off during testing.\n",
    "\n",
    "2. **Disable Gradient Calculation**:\n",
    "   - The `torch.no_grad()` context is used to disable gradient calculation. This makes the code more memory-efficient and faster, as gradient information is not needed during testing.\n",
    "\n",
    "3. **Iterate Over Test Dataset**:\n",
    "   - The loop iterates over batches of data provided by `test_loader`.\n",
    "     - **Data Preparation**:\n",
    "       - The target labels (`targets`) are moved to the appropriate device (CPU, GPU, or MPS).\n",
    "       - The input data (`data`) is converted to spike trains using `spikegen.rate()` with rate coding over `num_steps` time steps.\n",
    "       - The spike data is then reshaped to have the correct dimensions for the network's input.\n",
    "     - **Forward Pass**:\n",
    "       - The network processes the spike data, producing an output for each time step and corresponding membrane potentials (`mem`).\n",
    "     - **Prediction**:\n",
    "       - The outputs from all time steps are summed across time (`outputs.sum(dim=0)`), providing a cumulative activation for each class.\n",
    "       - The class with the highest activation is selected as the predicted label (`pred`).\n",
    "     - **Accuracy Calculation**:\n",
    "       - The predicted labels are compared to the true labels, and the number of correct predictions is accumulated in `correct`.\n",
    "       - The total number of samples processed is tracked in `total`.\n",
    "\n",
    "4. **Compute and Display Accuracy**:\n",
    "   - The overall accuracy of the model on the test dataset is calculated as the percentage of correct predictions (`100 * correct / total`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 65.80%\n"
     ]
    }
   ],
   "source": [
    "# Model test\n",
    "net.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data, targets in test_loader:\n",
    "        targets = targets.to(device)\n",
    "        spike_data = spikegen.rate(data, num_steps=num_steps)\n",
    "        spike_data = spike_data.to(device)\n",
    "        spike_data = spike_data.view(num_steps, data.size(0), -1)\n",
    "        outputs, mem = net(spike_data)\n",
    "        _, pred = outputs.sum(dim=0).max(1)\n",
    "        correct += (pred == targets).sum().item()\n",
    "        total += targets.size(0)\n",
    "\n",
    "print(f\"Accuracy: {100 * correct / total:.2f}%\")\n",
    "accuracy = 100. * correct / len(test_loader.dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
